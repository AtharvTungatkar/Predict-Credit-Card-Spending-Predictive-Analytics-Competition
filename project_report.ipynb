{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe23bad3",
   "metadata": {},
   "source": [
    "# Credit Card Spending Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025a18d",
   "metadata": {},
   "source": [
    "A credit card company has provided historical data on its active customers. The data includes demographic and behavioral information on customers. Your task is to build a model to predict monthly credit card spend for individual customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac9929",
   "metadata": {},
   "source": [
    "##### Initially started with Exploratory Data Analysis of the data. and Uncovered the distributions and relationships between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34bc39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Scatterplots with Monthly_spend\n",
    "for i in list(data.select_dtypes('number').columns):\n",
    "    sns.scatterplot(data=data,x=i,y='monthly_spend')\n",
    "    plt.show()\n",
    "\n",
    "# Histograms\n",
    "for i in list(data.select_dtypes('number').columns):\n",
    "    sns.histplot(data=data,x=i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e04ca",
   "metadata": {},
   "source": [
    "Used Ydata profiling to get an understanding as a part of EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b9f1d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "report=ProfileReport(df=eda)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6d084",
   "metadata": {},
   "source": [
    "# Submission 1\n",
    "Initially started with a Linear Regression Model with variables that were not having any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8a738",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_train = data.drop(labels=['monthly_spend','education_level','online_shopping_freq','utility_payment_count'],axis=1)\n",
    "y = data.monthly_spend\n",
    "categorical_variables=['gender','marital_status','region','employment_status','card_type']\n",
    "X= pd.get_dummies(X_train, \n",
    "                         prefix_sep = '_', \n",
    "                         columns = categorical_variables, \n",
    "                         drop_first = True)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X,y)\n",
    "reg.predict(X)\n",
    "\n",
    "\n",
    "# PREDICTION\n",
    "scoring_data = pd.read_csv('Datasets/scoring_data.csv')\n",
    "scoring_data.drop(labels=['education_level','online_shopping_freq','utility_payment_count'],axis=1,inplace=True)\n",
    "X_test = pd.get_dummies(scoring_data, \n",
    "                         prefix_sep = '_', \n",
    "                         columns = categorical_variables, \n",
    "                         drop_first = True)\n",
    "\n",
    "\n",
    "pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a6d43",
   "metadata": {},
   "source": [
    "# Submission 2\n",
    "Then Moved ahead with performing Linear Regression mmodel with imputing mode for categorical missing variables and median for quantitative variables having missing values as the data was right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a885a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data['education_level'].fillna(data['education_level'].mode()[0],inplace=True)\n",
    "data['online_shopping_freq'].fillna(data['online_shopping_freq'].median(),inplace=True)\n",
    "data['utility_payment_count'].fillna(data['utility_payment_count'].median(),inplace=True)\n",
    "\n",
    "X_train = data.drop(labels='monthly_spend',axis=1)\n",
    "y = data.monthly_spend\n",
    "categorical_variables=['gender','marital_status','education_level','region','employment_status','card_type']\n",
    "X= pd.get_dummies(X_train, \n",
    "                         prefix_sep = '_', \n",
    "                         columns = categorical_variables, \n",
    "                         drop_first = True)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X,y)\n",
    "reg.predict(X)\n",
    "\n",
    "# PREDICTION\n",
    "scoring_data = pd.read_csv('Datasets/scoring_data.csv')\n",
    "# Imputation\n",
    "\n",
    "\n",
    "scoring_data['education_level'].fillna(data['education_level'].mode()[0],inplace=True)\n",
    "scoring_data['online_shopping_freq'].fillna(data['online_shopping_freq'].median(),inplace=True)\n",
    "scoring_data['utility_payment_count'].fillna(data['utility_payment_count'].median(),inplace=True)\n",
    "\n",
    "X_test = pd.get_dummies(scoring_data, \n",
    "                         prefix_sep = '_', \n",
    "                         columns = categorical_variables, \n",
    "                         drop_first = True)\n",
    "\n",
    "pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c9cdbb",
   "metadata": {},
   "source": [
    "# Submission 3\n",
    "Moving Ahead with submission 3. I used best subset selection method  using same imputations mode and medians and fitted a multiple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a857c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Imputation\n",
    "data['education_level'].fillna(data['education_level'].mode()[0],inplace=True)\n",
    "data['online_shopping_freq'].fillna(data['online_shopping_freq'].median(),inplace=True)\n",
    "data['utility_payment_count'].fillna(data['utility_payment_count'].median(),inplace=True)\n",
    "\n",
    "# Splitting data and getting dummies\n",
    "X_train = data.drop(labels='monthly_spend',axis=1)\n",
    "y = data.monthly_spend\n",
    "categorical_variables=['gender','marital_status','education_level','region','employment_status','card_type']\n",
    "X= pd.get_dummies(X_train, \n",
    "                         prefix_sep = '_', \n",
    "                         columns = categorical_variables, \n",
    "                         drop_first = True)\n",
    "\n",
    "# Forward Selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sfs = SequentialFeatureSelector(LinearRegression(),\n",
    "                                  k_features='best',       \n",
    "                                  forward=True,\n",
    "                                  floating=False,          \n",
    "                                  scoring='r2',\n",
    "                                  cv=5)\n",
    "sfs = sfs.fit(X, y)\n",
    "pd.DataFrame(sfs.get_metric_dict()).T.loc[:,['feature_names','avg_score']].sort_values('avg_score', ascending = False)\n",
    "\n",
    "\n",
    "\n",
    "# Backward Elimination\n",
    "bfs = SequentialFeatureSelector(LinearRegression(),\n",
    "          k_features='best',       \n",
    "          forward=False,\n",
    "          floating=False,          \n",
    "          scoring='r2',\n",
    "          cv=5)\n",
    "bfs = bfs.fit(X, y)\n",
    "pd.DataFrame(sfs.get_metric_dict()).T.loc[:,['feature_names','avg_score']].sort_values('avg_score', ascending = False)\n",
    "\n",
    "# Stepwise\n",
    "\n",
    "stfs = SequentialFeatureSelector(LinearRegression(),\n",
    "          k_features='best',       \n",
    "          forward=True,\n",
    "          floating=True,           \n",
    "          scoring='r2',\n",
    "          cv=5)\n",
    "stfs = stfs.fit(X, y)\n",
    "pd.DataFrame(sfs.get_metric_dict()).T.loc[:,['feature_names','avg_score']].sort_values('avg_score', ascending = False)\n",
    "\n",
    "# All variations give same results\n",
    "\n",
    "variables=bfs.k_feature_names_\n",
    "variables=list(variables)\n",
    "variables\n",
    "\n",
    "# Fitting Regression model\n",
    "\n",
    "reg=LinearRegression()\n",
    "reg.fit(X[variables],y)\n",
    "reg.predict(X[variables])\n",
    "\n",
    "# PREDICTION\n",
    "scoring_data = pd.read_csv('Datasets/scoring_data.csv')\n",
    "# Imputation\n",
    "\n",
    "\n",
    "scoring_data['education_level'].fillna(data['education_level'].mode()[0],inplace=True)\n",
    "scoring_data['online_shopping_freq'].fillna(data['online_shopping_freq'].median(),inplace=True)\n",
    "scoring_data['utility_payment_count'].fillna(data['utility_payment_count'].median(),inplace=True)\n",
    "\n",
    "X_test = pd.get_dummies(scoring_data, \n",
    "                         prefix_sep = '_', \n",
    "                         columns = categorical_variables, \n",
    "                         drop_first = True)\n",
    "\n",
    "pred = reg.predict(X_test[variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24446141",
   "metadata": {},
   "source": [
    "# Submission 4\n",
    "In this submission I build on the submission 3 backward selection method. I used all variables that were used for submission 3. Then for numeric variables I perform EDA and then look for anytransformations.\n",
    "Majority of the variables were right skewed hence I applied log transformation to the right skewed variables.\n",
    "Basic EDA for categorical variables was done but we use them as they are from the submission 3.\n",
    "RMSE=277.70931827231027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5952a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "`missing_columns=['education_level','online_shopping_freq','utility_payment_count']\n",
    "\n",
    "eda=data[missing_columns]\n",
    "\n",
    "data['education_level'].fillna(data['education_level'].mode()[0],inplace=True)\n",
    "data['online_shopping_freq'].fillna(data['online_shopping_freq'].median(),inplace=True)\n",
    "`data['utility_payment_count'].fillna(data['utility_payment_count'].median(),inplace=True)\n",
    "\n",
    "data['credit_limit']=np.log(data['credit_limit'])\n",
    "data['reward_points_balance']=np.log(data['reward_points_balance'])\n",
    "data['num_transactions']=np.log(data['num_transactions']+1)\n",
    "data['avg_transaction_value']=np.log(data['avg_transaction_value'])\n",
    "data['travel_frequency']=np.log(data['travel_frequency']+1)\n",
    "data['online_shopping_freq']=np.log(data['online_shopping_freq']+1)\n",
    "data['utility_payment_count']=np.log(data['utility_payment_count']+1)\n",
    "\n",
    "variables=['owns_home',\n",
    " 'has_auto_loan',\n",
    " 'credit_score',\n",
    " 'credit_limit',\n",
    " 'tenure',\n",
    " 'num_transactions',\n",
    " 'avg_transaction_value',\n",
    " 'online_shopping_freq',\n",
    " 'reward_points_balance',\n",
    " 'travel_frequency',\n",
    " 'utility_payment_count',\n",
    " 'num_children',\n",
    " 'num_credit_cards',\n",
    " 'gender',\n",
    " 'marital_status',\n",
    " 'education_level',\n",
    " 'region',\n",
    " 'employment_status',\n",
    " 'card_type']\n",
    "\n",
    " X_train = data[variables]\n",
    "y = data.monthly_spend\n",
    "categorical_variables=['gender','marital_status','education_level','region','employment_status','card_type']\n",
    "X= pd.get_dummies(X_train, \n",
    "                         prefix_sep = '_', \n",
    "                         columns = categorical_variables, \n",
    "                         drop_first = True)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model=LinearRegression()\n",
    "model.fit(X,y)\n",
    "model.predict(X)\n",
    "\n",
    "# PREDICTION\n",
    "s_data = pd.read_csv('Datasets/scoring_data.csv')\n",
    "\n",
    "variables=['owns_home',\n",
    " 'has_auto_loan',\n",
    " 'credit_score',\n",
    " 'credit_limit',\n",
    " 'tenure',\n",
    " 'num_transactions',\n",
    " 'avg_transaction_value',\n",
    " 'online_shopping_freq',\n",
    " 'reward_points_balance',\n",
    " 'travel_frequency',\n",
    " 'utility_payment_count',\n",
    " 'num_children',\n",
    " 'num_credit_cards',\n",
    " 'gender',\n",
    " 'marital_status',\n",
    " 'education_level',\n",
    " 'region',\n",
    " 'employment_status',\n",
    " 'card_type']\n",
    "\n",
    "# Imputation\n",
    "scoring_data=s_data[variables]\n",
    "\n",
    "scoring_data['education_level'].fillna(data['education_level'].mode()[0],inplace=True)\n",
    "scoring_data['online_shopping_freq'].fillna(data['online_shopping_freq'].median(),inplace=True)\n",
    "scoring_data['utility_payment_count'].fillna(data['utility_payment_count'].median(),inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8d02a",
   "metadata": {},
   "source": [
    "# Submission 5\n",
    "I tried the MICE algorithm to fill in the missing values and then test it based on the best working submission 3. Train Test split was used and data was trained using the train dataset.\n",
    "Resulted in Lower test rmse score but not performed well in the actual scoring dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35f5b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y=data['monthly_spend']\n",
    "X=data.drop(labels=['monthly_spend'],axis=1)\n",
    "\n",
    "# Apply train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_raw,X_test_raw,y_train,y_test=train_test_split(X,y,train_size=0.8,random_state=2121)\n",
    "\n",
    "umeric_variables=X_train_raw.select_dtypes('number').columns\n",
    "categorical_columns = X_train_raw.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep numeric columns as they are\n",
    ")\n",
    "X_train_raw_encoded=preprocessor.fit_transform(X_train_raw)\n",
    "feature_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns))\n",
    "numeric_cols = X_train_raw.select_dtypes(include=['number']).columns.tolist()\n",
    "new_cols = feature_names + numeric_cols\n",
    "\n",
    "X_train_processed=pd.DataFrame(X_train_raw_encoded, columns=new_cols)\n",
    "\n",
    "estimator = RandomForestRegressor(n_estimators=10, random_state=2121)\n",
    "\n",
    "# 3. Create the Iterative Imputer (MICE)\n",
    "mice_imputer = IterativeImputer(\n",
    "    estimator=estimator,\n",
    "    max_iter=10,        # Number of imputation cycles (T)\n",
    "    initial_strategy='mean', # Initial placeholder strategy\n",
    "    random_state=2121\n",
    ")\n",
    "\n",
    "# 4. Fit the Imputer and Transform the Data\n",
    "# The imputer automatically handles all columns, using each as a predictor for the others.\n",
    "df_imputed_array = mice_imputer.fit_transform(X_train_processed)\n",
    "\n",
    "# Convert the resulting NumPy array back to a Pandas DataFrame\n",
    "df_imputed = pd.DataFrame(df_imputed_array, columns=X_train_processed.columns)\n",
    "\n",
    "# Lets try Stepwise\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "stfs = SequentialFeatureSelector(LinearRegression(),\n",
    "          k_features='best',       \n",
    "          forward=True,\n",
    "          floating=True,           \n",
    "          scoring='r2',\n",
    "          cv=5)\n",
    "stfs = stfs.fit(df_imputed, y_train)\n",
    "pd.DataFrame(stfs.get_metric_dict()).T.loc[:,['feature_names','avg_score']].sort_values('avg_score', ascending = False)\n",
    "\n",
    "X_train=df_imputed[list(stfs.k_feature_names_)]\n",
    "\n",
    "reg=LinearRegression().fit(X_train,y_train)\n",
    "\n",
    "# COnvert TEst Types\n",
    "\n",
    "numeric_variables=X_test_raw.select_dtypes('number').columns\n",
    "categorical_columns = X_test_raw.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "X_test_raw_encoded=preprocessor.transform(X_test_raw)\n",
    "feature_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns))\n",
    "numeric_cols = X_test_raw.select_dtypes(include=['number']).columns.tolist()\n",
    "new_cols = feature_names + numeric_cols\n",
    "\n",
    "X_test_processed=pd.DataFrame(X_test_raw_encoded, columns=new_cols)\n",
    "\n",
    "# Transform mice\n",
    "\n",
    "X_test_imputed_array = mice_imputer.transform(X_test_processed)\n",
    "X_test= pd.DataFrame(X_test_imputed_array, columns=X_test_processed.columns)\n",
    "X_test=X_test[list(stfs.k_feature_names_)]\n",
    "\n",
    "y_hat_test=reg.predict(X_test)\n",
    "\n",
    "scoring_data=pd.read_csv('Datasets/scoring_data.csv')\n",
    "\n",
    "numeric_variables=scoring_data.select_dtypes('number').columns\n",
    "categorical_columns = scoring_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "scoring_data_encoded=preprocessor.transform(scoring_data)\n",
    "feature_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns))\n",
    "numeric_cols = scoring_data.select_dtypes(include=['number']).columns.tolist()\n",
    "new_cols = feature_names + numeric_cols\n",
    "\n",
    "scoring_data_processed=pd.DataFrame(scoring_data_encoded, columns=new_cols)\n",
    "\n",
    "# Transform mice\n",
    "\n",
    "scoring_data_imputed_array = mice_imputer.transform(scoring_data_processed)\n",
    "scoring_data_pred= pd.DataFrame(scoring_data_imputed_array, columns=scoring_data_processed.columns)\n",
    "scoring_data_pred=scoring_data_pred[list(stfs.k_feature_names_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c0a6ee",
   "metadata": {},
   "source": [
    "# Submission 6\n",
    "Use the same method in submission 5 and train on full train dataset.\n",
    "It was able to reduce the score to the lowest of first 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd558a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y=data['monthly_spend']\n",
    "X=data.drop(labels=['monthly_spend'],axis=1)\n",
    "\n",
    "numeric_variables=X.select_dtypes('number').columns\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep numeric columns as they are\n",
    ")\n",
    "X_encoded=preprocessor.fit_transform(X)\n",
    "feature_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns))\n",
    "numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "new_cols = feature_names + numeric_cols\n",
    "\n",
    "X_processed=pd.DataFrame(X_encoded, columns=new_cols)\n",
    "\n",
    "estimator = RandomForestRegressor(n_estimators=10, random_state=2121)\n",
    "\n",
    "# 3. Create the Iterative Imputer (MICE)\n",
    "mice_imputer = IterativeImputer(\n",
    "    estimator=estimator,\n",
    "    max_iter=10,        # Number of imputation cycles (T)\n",
    "    initial_strategy='mean', # Initial placeholder strategy\n",
    "    random_state=2121\n",
    ")\n",
    "\n",
    "# 4. Fit the Imputer and Transform the Data\n",
    "# The imputer automatically handles all columns, using each as a predictor for the others.\n",
    "df_imputed_array = mice_imputer.fit_transform(X_processed)\n",
    "\n",
    "# Convert the resulting NumPy array back to a Pandas DataFrame\n",
    "df_imputed = pd.DataFrame(df_imputed_array, columns=X_processed.columns)\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "stfs = SequentialFeatureSelector(LinearRegression(),\n",
    "          k_features='best',       \n",
    "          forward=True,\n",
    "          floating=True,           \n",
    "          scoring='r2',\n",
    "          cv=5)\n",
    "stfs = stfs.fit(df_imputed, y)\n",
    "pd.DataFrame(stfs.get_metric_dict()).T.loc[:,['feature_names','avg_score']].sort_values('avg_score', ascending = False)\n",
    "\n",
    "\n",
    "\n",
    "X_train=df_imputed[list(stfs.k_feature_names_)]\n",
    "\n",
    "reg=LinearRegression().fit(X_train,y)\n",
    "y_hat=reg.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "root_mean_squared_error(y,y_hat)\n",
    "\n",
    "scoring_data=pd.read_csv('Datasets/scoring_data.csv')\n",
    "\n",
    "numeric_variables=scoring_data.select_dtypes('number').columns\n",
    "categorical_columns = scoring_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "scoring_data_encoded=preprocessor.transform(scoring_data)\n",
    "feature_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns))\n",
    "numeric_cols = scoring_data.select_dtypes(include=['number']).columns.tolist()\n",
    "new_cols = feature_names + numeric_cols\n",
    "\n",
    "scoring_data_processed=pd.DataFrame(scoring_data_encoded, columns=new_cols)\n",
    "\n",
    "# Transform mice\n",
    "\n",
    "scoring_data_imputed_array = mice_imputer.transform(scoring_data_processed)\n",
    "scoring_data_pred= pd.DataFrame(scoring_data_imputed_array, columns=scoring_data_processed.columns)\n",
    "scoring_data_pred=scoring_data_pred[list(stfs.k_feature_names_)]\n",
    "\n",
    "pred=reg.predict(scoring_data_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5486a70",
   "metadata": {},
   "source": [
    "# Submission 7 \n",
    "I used the same technique as the one above but the only change was I used median in regression imputation instead of mean. It didn't reduced the score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5313eeb1",
   "metadata": {},
   "source": [
    "# Submission 8\n",
    "STandardize variables before regression. Performed well but didn't beat the mean imputation with MICE in submission 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c590f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Datasets/analysis_data.csv')\n",
    "\n",
    "y = data['monthly_spend']\n",
    "X = data.drop(columns=['monthly_spend'])\n",
    "\n",
    "# Identify column types\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_columns = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# 1. OneHotEncoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "encoded_cat_cols = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns))\n",
    "new_cols = encoded_cat_cols + numeric_columns\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=new_cols)\n",
    "\n",
    "# 2. MICE Imputation\n",
    "mice = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(n_estimators=20, random_state=2121),\n",
    "    max_iter=10,\n",
    "    initial_strategy='mean',\n",
    "    random_state=2121\n",
    ")\n",
    "\n",
    "X_imputed = mice.fit_transform(X_encoded_df)\n",
    "X_imputed_df = pd.DataFrame(X_imputed, columns=new_cols)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# 3. Stepwise Feature Selection (on imputed data)\n",
    "stfs = SequentialFeatureSelector(\n",
    "    LinearRegression(),\n",
    "    k_features='best',\n",
    "    forward=True,\n",
    "    floating=True,\n",
    "    scoring='r2',\n",
    "    cv=5\n",
    ")\n",
    "stfs = stfs.fit(X_imputed_df, y)\n",
    "\n",
    "selected_features = list(stfs.k_feature_names_)\n",
    "X_train = X_imputed_df[selected_features]\n",
    "\n",
    "# 4. Scale selected features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 5. Fit Linear Regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_scaled, y)\n",
    "\n",
    "# 2. Impute using TRAINED MICE\n",
    "scoring_imputed = mice.transscoring_data = pd.read_csv('Datasets/scoring_data.csv')\n",
    "\n",
    "# Same categorical columns as training\n",
    "categorical_columns_scoring = scoring_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 1. Encode using TRAINED preprocessor\n",
    "scoring_encoded = preprocessor.transform(scoring_data)\n",
    "numeric_cols_scoring = scoring_data.select_dtypes(include=['number']).columns.tolist()\n",
    "new_cols_scoring = encoded_cat_cols + numeric_cols_scoring\n",
    "\n",
    "scoring_encoded_df = pd.DataFrame(scoring_encoded, columns=new_cols_scoring)\n",
    "\n",
    "form(scoring_encoded_df)\n",
    "scoring_imputed_df = pd.DataFrame(scoring_imputed, columns=new_cols_scoring)\n",
    "\n",
    "# 3. Select SAME stepwise features\n",
    "scoring_selected = scoring_imputed_df[selected_features]\n",
    "\n",
    "# 4. Scale with TRAINED scaler\n",
    "scoring_scaled = scaler.transform(scoring_selected)\n",
    "\n",
    "# 5. Predict\n",
    "pred = reg.predict(scoring_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb8fa17",
   "metadata": {},
   "source": [
    "# Submission 9\n",
    "Tried Ridge regression and mice imputation, worked well. Log transformed Y  then tried ridge worked poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd74f47",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Datasets/analysis_data.csv')\n",
    "y=data['monthly_spend']\n",
    "X=data.drop(labels=['monthly_spend'],axis=1)\n",
    "\n",
    "numeric_variables=X.select_dtypes('number').columns\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep numeric columns as they are\n",
    ")\n",
    "X_encoded=preprocessor.fit_transform(X)\n",
    "feature_names = list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_columns))\n",
    "numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "new_cols = feature_names + numeric_cols\n",
    "\n",
    "X_processed=pd.DataFrame(X_encoded, columns=new_cols)\n",
    "\n",
    "estimator = RandomForestRegressor(n_estimators=10, random_state=2121)\n",
    "\n",
    "# 3. Create the Iterative Imputer (MICE)\n",
    "mice_imputer = IterativeImputer(\n",
    "    estimator=estimator,\n",
    "    max_iter=10,        # Number of imputation cycles (T)\n",
    "    initial_strategy='mean', # Initial placeholder strategy\n",
    "    random_state=2121\n",
    ")\n",
    "\n",
    "# 4. Fit the Imputer and Transform the Data\n",
    "# The imputer automatically handles all columns, using each as a predictor for the others.\n",
    "df_imputed_array = mice_imputer.fit_transform(X_processed)\n",
    "\n",
    "# Convert the resulting NumPy array back to a Pandas DataFrame\n",
    "df_imputed = pd.DataFrame(df_imputed_array, columns=X_processed.columns)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train = df_imputed\n",
    "\n",
    "# 3. Ridge Regression with automatic alpha selection\n",
    "ridge = RidgeCV(alphas=[0.1, 1.0, 10.0, 50.0, 100.0], cv=5)\n",
    "ridge.fit(X_train, y)\n",
    "\n",
    "# 4. Predictions\n",
    "y_hat = ridge.predict(X_train)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_train2 = df_imputed\n",
    "y_train_log = np.log1p(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# 3. Ridge Regression with automatic alpha selection\n",
    "ridge2 = RidgeCV(alphas=[0.1, 1.0, 10.0, 50.0, 100.0], cv=5)\n",
    "ridge2.fit(X_train_scaled, y_train_log)\n",
    "\n",
    "# 4. Predictions\n",
    "y_hat2 = np.expm1(ridge2.predict(X_train_scaled))\n",
    "\n",
    "root_mean_squared_error(y,y_hat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a613b8",
   "metadata": {},
   "source": [
    "# Submisssion 10\n",
    "\n",
    "This was the best Model Tried to fit the polynomial terms and then penalize them using Elastic net regression. It was robust and always generated less RMSE for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a835fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "data = pd.read_csv(\"Datasets/analysis_data.csv\")\n",
    "\n",
    "y = data[\"monthly_spend\"]\n",
    "X = data.drop(columns=[\"monthly_spend\"])\n",
    "\n",
    "categorical_columns = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_columns = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# OneHot Encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_columns)\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "encoded_cat_cols = preprocessor.named_transformers_[\"cat\"].get_feature_names_out(categorical_columns)\n",
    "new_cols = list(encoded_cat_cols) + numeric_columns\n",
    "\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=new_cols)\n",
    "\n",
    "# MICE Imputation using BayesianRidge\n",
    "mice = IterativeImputer(\n",
    "    estimator=BayesianRidge(),\n",
    "    max_iter=10,\n",
    "    initial_strategy='median',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_imputed = mice.fit_transform(X_encoded_df)\n",
    "X_imputed_df = pd.DataFrame(X_imputed, columns=new_cols)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Polynomial features \n",
    "poly = PolynomialFeatures(\n",
    "    degree=2,\n",
    "    interaction_only=True,   \n",
    "    include_bias=False\n",
    ")\n",
    "\n",
    "X_poly = poly.fit_transform(X_imputed_df)\n",
    "poly_feature_names = poly.get_feature_names_out(X_imputed_df.columns)\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "enet = ElasticNetCV(\n",
    "    l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    alphas=[0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    max_iter=5000\n",
    ")\n",
    "\n",
    "enet.fit(X_poly, y)\n",
    "\n",
    "coef = enet.coef_\n",
    "features = poly.get_feature_names_out(new_cols)\n",
    "\n",
    "significant_vars = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"coef\": coef\n",
    "})\n",
    "\n",
    "significant_vars = significant_vars[significant_vars[\"coef\"] != 0]\n",
    "significant_vars = significant_vars.sort_values(\"coef\", ascending=False)\n",
    "\n",
    "scoring = pd.read_csv(\"Datasets/scoring_data.csv\")\n",
    "\n",
    "# Apply same OneHot encoder\n",
    "X_scoring_encoded = preprocessor.transform(scoring)\n",
    "numeric_scoring_cols = scoring.select_dtypes(include=['number']).columns.tolist()\n",
    "X_scoring_encoded_df = pd.DataFrame(X_scoring_encoded, columns=new_cols)\n",
    "\n",
    "# MICE imputation\n",
    "X_scoring_imputed = mice.transform(X_scoring_encoded_df)\n",
    "X_scoring_imputed_df = pd.DataFrame(X_scoring_imputed, columns=new_cols)\n",
    "\n",
    "# Polynomial features\n",
    "X_scoring_poly = poly.transform(X_scoring_imputed_df)\n",
    "\n",
    "# Predict\n",
    "pred = enet.predict(X_scoring_poly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5cdee",
   "metadata": {},
   "source": [
    "# Submission 11\n",
    "Tried Polynomial regression with Degree 3 with ELastic Net but didn't work as the RMSE increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d493ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Polynomial features (degree=2 interactions only)\n",
    "poly = PolynomialFeatures(\n",
    "    degree=3,\n",
    "    interaction_only=True,   # avoids squared terms if you want\n",
    "    include_bias=False\n",
    ")\n",
    "\n",
    "X_poly = poly.fit_transform(X_imputed_df)\n",
    "poly_feature_names = poly.get_feature_names_out(X_imputed_df.columns)\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# Elastic Net without scaling X or y\n",
    "enet = ElasticNetCV(\n",
    "    # l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    # alphas=[0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    l1_ratio=[0.9],\n",
    "    alphas=[10],\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    max_iter=5000\n",
    ")\n",
    "\n",
    "enet.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3365974",
   "metadata": {},
   "source": [
    "I also fitted many Machine Learning models but they always overfitted the data.\n",
    "\n",
    "\n",
    "Github : https://github.com/AtharvTungatkar/Predict-Credit-Card-Spending-Predictive-Analytics-Competition"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
